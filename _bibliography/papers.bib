---
---
@article{li20243dgm,
  selected={true},   
  abbr={Preprint},
  preview={3dgm.png},
  title={Memorize What Matters: Emergent Scene Decomposition from Multitraverse}, 
  author={Li, Yiming and Wang, Zehong and Wang, Yue and Yu, Zhiding and Gojcic, Zan and Pavone, Marco and Feng, Chen and Alvarez, Jose M},
  journal={arXiv preprint arXiv:2405.17187},
  year={2024},
  code={https://github.com/NVlabs/3DGM},
  pdf={https://arxiv.org/pdf/2405.17187},
  website ={https://nvlabs.github.io/3DGM/},
  arxiv={2405.17187},
  abstract={Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust differentiable rendering problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residuals mining, and robust optimization, 3DGM jointly performs 3D mapping and 2D segmentation without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.}
}

@article{li2023sscbench,
  selected={true},   
  abbr={Preprint},
  preview={sscbench.gif},
  title={SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving},
  author={Li, Yiming and Li, Sihang and Liu, Xinhao and Gong, Moonjun and Li, Kenan and Chen, Nuo and Wang, Zijun and Li, Zhiheng and Jiang, Tao and Yu, Fisher and others},
  journal={arXiv preprint arXiv:2306.09001},
  year={2024},
  code={https://github.com/ai4ce/SSCBench},
  pdf={https://arxiv.org/pdf/2306.09001.pdf},
  arxiv={2306.09001},
  abstract={Semantic scene completion (SSC) is crucial for holistic 3D scene understanding by jointly estimating semantics and geometry from sparse observations. However, progress in SSC, particularly in autonomous driving scenarios, is hindered by the scarcity of high-quality datasets. To overcome this challenge, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely-used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of the camera- and LiDAR-based SSC across various real-world scenarios. We present quantitative and qualitative evaluations of state-of-the-art algorithms on SSCBench and commit to continuously incorporating novel automotive datasets and SSC algorithms to drive further advancements in this field.}
}

@InProceedings{li2024mars,
  selected={true},  
  abbr={CVPR},
  preview={mars.png},
  title={Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset}, 
  author={Li, Yiming and Li, Zhiheng and Chen, Nuo and Gong, Moonjun and Lyu, Zonglin and Wang, Zehong and Jiang, Peili and Feng, Chen},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024},
  abstract={Large-scale datasets have fueled recent advancements in AI-based autonomous vehicle research. However, these datasets are usually collected from a single vehicle's one-time pass of a certain location, lacking multiagent interactions or repeated traversals of the same place. Such information could lead to transformative enhancements in autonomous vehicles' perception, prediction, and planning capabilities. To bridge this gap, in collaboration with the self-driving company May Mobility, we present MARS dataset which unifies scenarios that enable MultiAgent, multitraveRSal, and multimodal autonomous vehicle research. More specifically, MARS is collected with a fleet of autonomous vehicles driving within a certain geographical area. Each vehicle has its own route and different vehicles may appear at nearby locations. Each vehicle is equipped with a LiDAR and surround-view RGB cameras. We curate two subsets in MARS: one facilitates collaborative driving with multiple vehicles simultaneously present at the same location, and the other enables memory retrospection through asynchronous traversals of the same location by multiple vehicles. We conduct experiments in place recognition and neural reconstruction. More importantly, MARS introduces new research opportunities and challenges such as multitraversal 3D reconstruction, multiagent perception, and unsupervised object discovery. Our data and codes can be found at https://ai4ce.github.io/MARS/.}
}

@article{su2023collaborative,
  abbr={Preprint},
  preview={comot.png},
  title={Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation},
  author={Su, Sanbao and Han, Songyang and Li, Yiming and Zhang, Zhili and Feng, Chen and Ding, Caiwen and Miao, Fei},
  journal={arXiv preprint arXiv:2303.14346},
  year={2023},
  arxiv={2303.14346},
  pdf={https://arxiv.org/pdf/2303.14346.pdf},
  website={https://coperception.github.io/MOT-CUP/},
  abstract={Object detection and multiple object tracking (MOT) are essential components of self-driving systems. Accurate detection and uncertainty quantification are both critical for onboard modules, such as perception, prediction, and planning, to improve the safety and robustness of autonomous vehicles. Collaborative object detection (COD) has been proposed to improve detection accuracy and reduce uncertainty by leveraging the viewpoints of multiple agents. However, little attention has been paid on how to leverage the uncertainty quantification from COD to enhance MOT performance. In this paper, as the first attempt, we design the uncertainty propagation framework to address this challenge, called MOT-CUP. Our framework first quantifies the uncertainty of COD through direct modeling and conformal prediction, and propogates this uncertainty information during the motion prediction and association steps. MOT-CUP is designed to work with different collaborative object detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, a comprehensive collaborative perception dataset, and demonstrate a 2% improvement in accuracy and a 2.67X reduction in uncertainty compared to the baselines, e.g., SORT and ByteTrack. MOT-CUP demonstrates the importance of uncertainty quantification in both COD and MOT, and provides the first attempt to improve the accuracy and reduce the uncertainty in MOT based on COD through uncertainty propogation.}
}

@inproceedings{li2023amongus,
  selected={true},  
  abbr={ICCV},
  preview={amongus.png},    
  title={Among Us: Adversarially Robust Collaborative Perception by Consensus},
  author={Li, Yiming and Fang, Qi and Bai, Jiamu and Chen, Siheng and Juefei-Xu, Felix and Feng, Chen},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2023},
  pdf={https://arxiv.org/pdf/2303.09495.pdf},
  code = {https://github.com/coperception/ROBOSAC},
  arxiv= {2303.09495},
  abstract = {Multiple robots could perceive a scene (e.g., detect objects) collaboratively better than individuals, although easily suffer from adversarial attacks when using deep learning. This could be addressed by the adversarial defense, but its training requires the often-unknown attacking mechanism. Differently, we propose ROBOSAC, a novel sampling-based defense strategy generalizable to unseen attackers. Our key idea is that collaborative perception should lead to consensus rather than dissensus in results compared to individual perception. This leads to our hypothesize-and-verify framework: perception results with and without collaboration from a random subset of teammates are compared until reaching a consensus. In such a framework, more teammates in the sampled subset often entail better perception performance but require longer sampling time to reject potential attackers. Thus, we derive how many sampling trials are needed to ensure the desired size of an attacker-free subset, or equivalently, the maximum size of such a subset that we can successfully sample within a given number of trials. We validate our method on the task of collaborative 3D object detection in autonomous driving scenarios.}
}

@inproceedings{Li2023iccv, 
  abbr={ICCV},
  preview={pvt++.gif},
  author={Li, Bowen and Huang, Ziyuan and Ye, Junjie and Li, Yiming and Scherer, Sebastian and Zhao, Hang and Fu, Changhong},   
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, 
  title={{PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework}},
  year={2023},
  arxiv={2211.11629},
  website ={https://jaraxxus-me.github.io/ICCV2023_PVTpp/},
  code = {https://github.com/Jaraxxus-Me/PVT_pp},
  abstract={Visual object tracking is essential to intelligent robots. Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicles (UAVs), where robust tracking is more challenging and onboard computation is limited, the latency issue can be fatal. In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). Unlike existing solutions that naively append Kalman Filters after trackers, PVT++ can be jointly optimized, so that it takes not only motion information but can also leverage the rich visual knowledge in most pre-trained tracker models for robust prediction. Besides, to bridge the training-evaluation domain gap, we propose a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. These careful designs have made the small-capacity lightweight PVT++ a widely effective solution. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting. Empirical results on a robotic platform from the aerial perspective show that PVT++ can achieve significant performance gain on various trackers and exhibit higher accuracy than prior solutions, largely mitigating the degradation brought by latency.}
  }

@INPROCEEDINGS{HeRSS23, 
  selected={true},   
  abbr={RSS},
  preview={deepexplorer.png},
  AUTHOR    = {He, Yuhang and Fang, Irving and Li, Yiming and Shah, Rushi Bhavesh and Feng, Chen}, 
  TITLE     = {{Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space}}, 
  BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
  YEAR      = {2023},
  pdf = {https://www.roboticsproceedings.org/rss19/p099.pdf},
  code = {https://github.com/ai4ce/DeepExplorer},
  website = {https://ai4ce.github.io/DeepExplorer/},
  slides = {https://ai4ce.github.io/DeepExplorer/static/videos/Simplified_ATM.mp4},
  abstract = {We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal. The two planners are jointly trained via deeply-supervised imitation learning from expert demonstrations. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. The resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. }

} 

@InProceedings{li2023voxformer,
  selected={true},  
  abbr={CVPR},
  preview={voxformer.png},
  title={VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion},
  author={Li, Yiming and Yu, Zhiding and Choy, Christopher and Xiao, Chaowei and Alvarez, Jose M and Fidler, Sanja and Feng, Chen and Anandkumar, Anima},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition <b><font color="firebrick">(highlight, top 2.5%)</font></b>},
  pages={9087--9098},
  year={2023},
  code = {https://github.com/NVlabs/VoxFormer},
  slides = {https://www.youtube.com/watch?v=L0M9ayR316g},
  pdf={https://openaccess.thecvf.com/content/CVPR2023/papers/Li_VoxFormer_Sparse_Voxel_Transformer_for_Camera-Based_3D_Semantic_Scene_Completion_CVPR_2023_paper.pdf},
  abstract={Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformerbased semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show that VoxFormer outperforms the state of the art with a relative improvement of 20.0% in geometry and 18.1% in semantics and reduces GPU memory during training to less than 16GB. Our code is available on https://github.com/NVlabs/VoxFormer.}
}

@inproceedings{chen2022deepmapping2,
  selected={true},  
  abbr={CVPR},
  preview={dm2.gif},
  title={DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization},
  author={Chen, Chao and Liu, Xinhao and Li, Yiming and Ding, Li and Feng, Chen},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9306--9316},
  year={2023},
  code = {https://github.com/ai4ce/DeepMapping2},
  website = {https://ai4ce.github.io/DeepMapping2/},
  pdf={https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DeepMapping2_Self-Supervised_Large-Scale_LiDAR_Map_Optimization_CVPR_2023_paper.pdf},
  abstract={LiDAR mapping is important yet challenging in self-driving and mobile robotics. To tackle such a global point cloud registration problem, DeepMapping [1] converts the complex map estimation into a self-supervised training of simple deep networks. Despite its broad convergence range on small datasets, DeepMapping still cannot produce satisfactory results on large-scale datasets with thousands of frames. This is due to the lack of loop closures and exact cross-frame point correspondences, and the slow convergence of its global localization network. We propose DeepMapping2 by adding two novel techniques to address these issues: (1) organization of training batch based on map topology from loop closing, and (2) self-supervised local-to-global point consistency loss leveraging pairwise registration. Our experiments and ablation studies on public datasets such as KITTI, NCLT, and Nebula demonstrate the effectiveness of our method.}
  }

@inproceedings{su2022uncertainty,
  abbr={ICRA},
  preview={uacodet.png},
  title={Uncertainty Quantification of Collaborative Detection for Self-Driving},
  author={Su, Sanbao and Li, Yiming and He, Sihong and Han, Songyang and Feng, Chen and Ding, Caiwen and Miao, Fei},
  booktitle={IEEE International Conference on Robotics and Automation},
  year={2023},
  code = {https://github.com/coperception/double-m-quantification},
  website = {https://coperception.github.io/double-m-quantification/},
  abstract={Sharing information between connected and autonomous vehicles (CAVs) fundamentally improves the performance of collaborative object detection for self-driving. However, CAVs still have uncertainties on object detection due to practical challenges, which will affect the later modules in self-driving such as planning and control. Hence, uncertainty quantification is crucial for safety-critical systems such as CAVs. Our work is the first to estimate the uncertainty of collaborative object detection. We propose a novel uncertainty quantification method, called Double-M Quantification, which tailors a moving block bootstrap (MBB) algorithm with direct modeling of the multivariant Gaussian distribution of each corner of the bounding box. Our method captures both the epistemic uncertainty and aleatoric uncertainty with one inference pass based on the offline Double-M training process. And it can be used with different collaborative object detectors. Through experiments on the comprehensive collaborative perception dataset, we show that our Double-M method achieves more than 4× improvement on uncertainty score and more than 3% accuracy improvement, compared with the state-of-the-art uncertainty quantification methods.}
  
}

@article{zuo2023adversarial,
  abbr={RA-L},
  preview={abdnet.png},
  title={Adversarial Blur-Deblur Network for Robust UAV Tracking},
  author={Zuo, Haobo and Fu, Changhong and Li, Sihang and Lu, Kunhan and Li, Yiming and Feng, Chen},
  journal={IEEE Robotics and Automation Letters},
  volume={8},
  number={2},
  pages={1101--1108},
  year={2023},
  publisher={IEEE},
  code={https://github.com/vision4robotics/ABDNet},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10015799},
  abstract={Unmanned aerial vehicle (UAV) tracking has been widely applied in real-world applications such as surveillance and monitoring. However, the inherent high maneuverability and agility of UAV often lead to motion blur, which can impair the visual appearance of the target object and easily degrade the existing trackers. To overcome this challenge, this work proposes a tracking-oriented adversarial blur-deblur network (ABDNet), composed of a novel deblurrer to recover the visual appearance of the tracked object, and a brand-new blur generator to produce realistic blurry images for adversarial training. More specifically, the deblurrer progressively refines the features through pixel-wise, spatial-wise, and channel-wise stages to achieve excellent deblurring performance. The blur generator adaptively fuses an image sequence with a learnable kernel to create realistic blurry images. During training, ABDNet is plugged into the state-of-the-art real-time trackers and trained with blurring-deblurring loss as well as tracking loss. During inference, the blur generator is removed, while the deblurrer and the tracker can work together for UAV tracking. Extensive experiments in both public datasets and real-world testing have validated the effectiveness of ABDNet.}
}

@article{li2023boosting,
  abbr={RA-L},
  preview={trtrack.png},
  title={Boosting UAV Tracking with Voxel-based Trajectory-Aware Pre-Training},
  author={Li, Sihang and Fu, Changhong and Lu, Kunhan and Zuo, Haobo and Li, Yiming and Feng, Chen},
  journal={IEEE Robotics and Automation Letters},
  volume={8},
  number={2},
  pages={1133--1140},
  year={2023},
  publisher={IEEE},
  code={https://github.com/vision4robotics/TRTrack},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10015867},
  abstract={Siamese network-based object tracking has remarkably promoted the automatic capability for highlymaneuvered unmanned aerial vehicles (UAVs). However, the leading-edge tracking framework often depends on template matching, making it trapped when facing multiple views of object in consecutive frames. Moreover, the general imagelevel pretrained backbone can overfit to holistic representations, causing the misalignment to learn object-level properties in UAV tracking. To tackle these issues, this work presents TRTrack, a comprehensive framework to fully exploit the stereoscopic representation for UAV tracking. Specifically, a novel pretraining paradigm method is proposed. Through trajectoryaware reconstruction training, the capability of the backbone to extract stereoscopic structure feature is strengthened without any parameter increment. Accordingly, an innovative hierarchical self-attention Transformer is proposed to capture the local detail information and global structure knowledge. For optimizing the correlation map, we proposed a novel spatial correlation refinement (SCR) module, which promotes the capability of modeling the long-range spatial dependencies. Comprehensive experiments on three UAV challenging benchmarks demonstrate that the proposed TRTrack achieves superior UAV tracking performance in both precision and efficiency. Quantitative tests in real-world settings fully prove the effectiveness of our work.}
}


@article{chen2022self,  
  abbr={Preprint},
  preview={tfvpr.gif},
  title={Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods},
  author={Chen, Chao and Liu, Xinhao and Xu, Xuchu and Li, Yiming and Ding, Li and Wang, Ruoyu and Feng, Chen},
  journal={arXiv preprint arXiv:2208.09315},
  year={2022},
  website={https://ai4ce.github.io/TF-VPR/},
  code={https://github.com/ai4ce/TF-VPR},
  arxiv={2208.09315},
  abstract={Visual place recognition (VPR) using deep networks has achieved state-of-the-art performance. However, most of the related approaches require a training set with ground truth sensor poses to obtain the positive and negative samples of each observation's spatial neighborhoods. When such knowledge is unknown, the temporal neighborhoods from a sequentially collected data stream could be exploited for self-supervision, although with suboptimal performance. Inspired by noisy label learning, we propose a novel self-supervised VPR framework that uses both the temporal neighborhoods and the learnable feature neighborhoods to discover the unknown spatial neighborhoods. Our method follows an iterative training paradigm which alternates between: (1) representation learning with data augmentation, (2) positive set expansion to include the current feature space neighbors, and (3) positive set contraction via geometric verification. We conduct comprehensive experiments on both simulated and real datasets, with input of both images and point clouds. The results demonstrate that our method outperforms the baselines in both recall rate, robustness, and a novel metric we proposed for VPR, the orientation diversity.}
}

@inproceedings{li2023multi,
  abbr={CoRL},
  preview={star.png},
  title={Multi-Robot Scene Completion: Towards Task-Agnostic Collaborative Perception},
  author={Li, Yiming and Zhang, Juexiao and Ma, Dekun and Wang, Yue and Feng, Chen},
  booktitle={6th Conference on Robot Learning},
  pages={2062--2072},
  year={2022},
  organization={PMLR},
  pdf={https://proceedings.mlr.press/v205/li23e/li23e.pdf},
  code={https://github.com/coperception/star},
  website={https://coperception.github.io/star/},
  abstract={Collaborative perception learns how to share information among multiple robots to perceive the environment better than individually done. Past research on this has been task-specific, such as detection or segmentation. Yet this leads to different information sharing for different tasks, hindering the large-scale deployment of collaborative perception. We propose the first task-agnostic collaborative perception paradigm that learns a single collaboration module in a self-supervised manner for different downstream tasks. This is done by a novel task termed multi-robot scene completion, where each robot learns to effectively share information for reconstructing a complete scene viewed by all robots. Moreover, we propose a spatiotemporal autoencoder (STAR) that amortizes over time the communication cost by spatial sub-sampling and temporal mixing. Extensive experiments validate our method’s effectiveness on scene completion and collaborative perception in autonomous driving scenarios.}
}


@InProceedings{Li_2022_CVPR,
  selected={true}, 
  abbr={CVPR},
  preview={egopat3d.png},
  title = {Egocentric Prediction of Action Target in 3D},
  author = {Li, Yiming and Cao, Ziang and Liang, Andrew and Liang, Benjamin and Chen, Luoyao and Zhao, Hang and Feng, Chen},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  month = {June},
  year = {2022},
  pdf={https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Egocentric_Prediction_of_Action_Target_in_3D_CVPR_2022_paper.pdf},
  code={https://github.com/ai4ce/EgoPAT3D},
  website={https://ai4ce.github.io/EgoPAT3D/},
  abstract={We are interested in anticipating as early as possible the target location of a person's object manipulation action in a 3D workspace from egocentric vision. It is important in fields like human-robot collaboration, but has not yet received enough attention from vision and learning communities. To stimulate more research on this challenging egocentric vision task, we propose a large multimodality dataset of more than 1 million frames of RGB-D and IMU streams, and provide evaluation metrics based on our high-quality 2D and 3D labels from semi-automatic annotation. Meanwhile, we design baseline methods using recurrent neural networks (RNNs) and conduct various ablation studies to validate their effectiveness. Our results demonstrate that this new task is worthy of further study by researchers in robotics, vision, and learning communities.}

}

@article{li2022v2x,
  abbr={RA-L},
  preview={v2x-sim.png},
  title={V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving},
  author={Li, Yiming and Ma, Dekun and An, Ziyan and Wang, Zixun and Zhong, Yiqi and Chen, Siheng and Feng, Chen},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={4},
  pages={10914--10921},
  year={2022},
  publisher={IEEE},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9835036},
  code={https://github.com/ai4ce/V2X-Sim},
  website={https://ai4ce.github.io/V2X-Sim/},
  abstract={Vehicle-to-everything (V2X) communication techniques enable the collaboration between vehicles and many other entities in the neighboring environment, which could fundamentally improve the perception system for autonomous driving. However, the lack of a public dataset significantly restricts the research progress of collaborative perception. To fill this gap, we present V2X-Sim, a comprehensive simulated multi-agent perception dataset for V2X-aided autonomous driving. V2X-Sim provides: (1) multi-agent sensor recordings from the road-side unit (RSU) and multiple vehicles that enable collaborative perception, (2) multi-modality sensor streams that facilitate multi-modality perception, and (3) diverse ground truths that support various perception tasks. Meanwhile, we build an open-source testbed and provide a benchmark for the state-of-the-art collaborative perception algorithms on three tasks, including detection, tracking and segmentation. V2X-Sim seeks to stimulate collaborative perception research for autonomous driving before realistic datasets become widely available.}
}


@inproceedings{li2021learning,
  selected={true},
  abbr={NeurIPS},
  preview={disconet.png},
  title={Learning Distilled Collaboration Graph for Multi-Agent Perception},
  author={Li, Yiming and Ren, Shunli and Wu, Pengxiang and Chen, Siheng and Feng, Chen and Zhang, Wenjun},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29541--29552},
  year={2021},
  code={https://github.com/ai4ce/DiscoNet},
  arxiv={2111.00643},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2021/file/f702defbc67edb455949f46babab0c18-Paper.pdf},
  abstract={To promote better performance-bandwidth trade-off for multi-agent perception, we propose a novel distilled collaboration graph (DiscoGraph) to model trainable, pose-aware, and adaptive collaboration among agents. Our key novelties lie in two aspects. First, we propose a teacher-student framework to train DiscoGraph via knowledge distillation. The teacher model employs an early collaboration with holistic-view inputs; the student model is based on intermediate collaboration with single-view inputs. Our framework trains DiscoGraph by constraining post-collaboration feature maps in the student model to match the correspondences in the teacher model. Second, we propose a matrix-valued edge weight in DiscoGraph. In such a matrix, each element reflects the inter-agent attention at a specific spatial region, allowing an agent to adaptively highlight the informative regions. During inference, we only need to use the student model named as the distilled collaboration network (DiscoNet). Attributed to the teacher-student framework, multiple agents with the shared DiscoNet could collaboratively approach the performance of a hypothetical teacher model with a holistic view. Our approach is validated on V2X-Sim 1.0, a large-scale multi-agent perception dataset that we synthesized using CARLA and SUMO co-simulation. Our quantitative and qualitative experiments in multi-agent 3D object detection show that DiscoNet could not only achieve a better performance-bandwidth trade-off than the state-of-the-art collaborative perception methods, but also bring more straightforward design rationale. }
}

@inproceedings{li2021fooling,
  selected={true},
  abbr={ICCV},
  preview={flat.gif},
  title={Fooling LiDAR Perception via Adversarial Trajectory Perturbation},
  author={Li, Yiming and Wen, Congcong and Juefei-Xu, Felix and Feng, Chen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision <b><font color="firebrick">(oral, top 3.0%)</font></b>},
  pages={7898--7907},
  year={2021},
  code={https://github.com/ai4ce/FLAT},
  pdf={https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Fooling_LiDAR_Perception_via_Adversarial_Trajectory_Perturbation_ICCV_2021_paper.pdf},
  website={https://ai4ce.github.io/FLAT/},
  abstract={LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When autonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estimation that is susceptible to wireless spoofing? We demonstrate such possibilities for the first time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spoofing of a self-driving car's trajectory with small perturbations is enough to make safety-critical objects undetectable or detected with incorrect positions. Moreover, polynomial trajectory perturbation is developed to achieve a temporally-smooth and highly-imperceptible attack. Extensive experiments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art detectors effectively, but also transfer to other detectors, raising a red flag for the community.}
}

@inproceedings{cao2021hift,
  abbr={ICCV},
  preview={hift.png},
  title={HiFT: Hierarchical Feature Transformer for Aerial Tracking},
  author={Cao, Ziang and Fu, Changhong and Ye, Junjie and Li, Bowen and Li, Yiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15457--15466},
  year={2021},
  arxiv={2108.00202},
  code={https://github.com/vision4robotics/HiFT},
  pdf={https://openaccess.thecvf.com/content/ICCV2021/papers/Cao_HiFT_Hierarchical_Feature_Transformer_for_Aerial_Tracking_ICCV_2021_paper.pdf},
  abstract={Most existing Siamese-based tracking methods execute the classification and regression of the target object based on the similarity maps. However, they either employ a single map from the last convolutional layer which degrades the localization accuracy in complex scenarios or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. Thus, in this work, we propose an efficient and effective hierarchical feature transformer (HiFT) for aerial tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into the feature transformer to achieve the interactive fusion of spatial (shallow layers) and semantics cues (deep layers). Consequently, not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can efficiently learn the interdependencies among multi-level features, thereby discovering a tracking-tailored feature space with strong discriminability. Comprehensive evaluations on four aerial benchmarks have proven the effectiveness of HiFT. Real-world tests on the aerial platform have strongly validated its practicability with a real-time speed.}
}

@inproceedings{cao2021siamapn++,
  abbr={IROS},
  preview={siamapn++.png},
  title={SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking},
  author={Cao, Ziang and Fu, Changhong and Ye, Junjie and Li, Bowen and Li, Yiming},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={3086--3092},
  year={2021},
  organization={IEEE},
  pdf={https://arxiv.org/pdf/2106.08816.pdf},
  code={https://github.com/vision4robotics/SiamAPN},
  abstract={Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-theart (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, e.g., severe occlusion and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, we conduct a special attentional aggregation network (AAN) consisting of self-AAN and cross-AAN for raising the representation ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the crossinterdependencies of two different semantic features including the location information of anchors. In addition, the anchor proposal network based on dual features is proposed to raise its robustness of tracking objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed.}
}

@inproceedings{fu2021siamese,
  abbr={ICRA},
  preview={siamapn.png},
  title={Siamese Anchor Proposal Network for High-Speed Aerial Tracking},
  author={Fu, Changhong and Cao, Ziang and Li, Yiming and Ye, Junjie and Feng, Chen},
  booktitle={IEEE International Conference on Robotics and Automation},
  pages={510--516},
  year={2021},
  organization={IEEE},
  pdf={https://arxiv.org/pdf/2012.10706.pdf},
  code={https://github.com/vision4robotics/SiamAPN},
  abstract={In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency. Therefore, their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV) is impeded. In this work, a novel two-stage Siamese network-based method is proposed for aerial tracking, \textit{i.e.}, stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) increase the robustness and generalization to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the superior performance of our approach, with a speed of around 200 frames/s.}
}


@inproceedings{li2020autotrack,
  abbr={CVPR},
  preview={autotrack.png},
  title={AutoTrack: Towards High-Performance Visual Tracking for UAV with Automatic Spatio-Temporal Regularization},
  author={Li, Yiming and Fu, Changhong and Ding, Fangqiang and Huang, Ziyuan and Lu, Geng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11923--11932},
  year={2020},
  pdf={https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_AutoTrack_Towards_High-Performance_Visual_Tracking_for_UAV_With_Automatic_Spatio-Temporal_CVPR_2020_paper.pdf},
  code={https://github.com/vision4robotics/AutoTrack},
  abstract={Most existing trackers based on discriminative correlation filters (DCF) try to introduce predefined regularization term to improve the learning of target objects, e.g., by suppressing background learning or by restricting change rate of correlation filters. However, predefined parameters introduce much effort in tuning them and they still fail to adapt to new situations that the designer didn’t think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term. Spatially local response map variation is introduced as spatial regularization to make DCF focus on the learning of trust-worthy parts of the object, and global response map variation determines the updating rate of the filter. Extensive experiments on four UAV benchmarks, i.e., DTB70, UAVDT, UAV123@10fps and VisDrone-test-dev, have proven that our tracker performs favorably against the state-of-theart CPU- and GPU-based trackers, with average speed of 59.2 frames per second (FPS) running on a single CPU. Our tracker is additionally proposed to be applied to localize the moving camera. Considerable tests in the indoor practical scenarios have proven the effectiveness and versatility of our localization method.}
}

@inproceedings{li2020augmented,
  abbr={IROS},
  preview={amcf.png},
  title={Augmented Memory for Correlation Filters in Real-Time UAV Tracking},
  author={Li, Yiming and Fu, Changhong and Ding, Fangqiang and Huang, Ziyuan and Pan, Jia},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={1559--1566},
  year={2020},
  organization={IEEE},
  code={https://github.com/vision4robotics/AMCF-tracker},
  video={https://www.youtube.com/watch?v=CGH5o2J1ohI},
  pdf={http://ras.papercept.net/images/temp/IROS/files/0869.pdf},
  abstract={The outstanding computational efficiency of discriminative correlation filter (DCF) fades away with various complicated improvements. Previous appearances are also gradually forgotten due to the exponential decay of historical views in traditional appearance updating scheme of DCF framework, reducing the model’s robustness. In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at real-time speed. Several historical views and the current view are simultaneously introduced in training to allow the tracker to adapt to new appearances as well as memorize previous ones. A novel rapid compressed context learning is proposed to increase the discriminative ability of the filter efficiently. Substantial experiments on UAVDT and UAV123 datasets have validated that the proposed tracker performs competitively against other 26 top DCF and deep-based trackers with over 40fps on CPU.}
}

@inproceedings{ding2020automatic,
  abbr={IROS},
  preview={autof.png},
  title={Automatic failure recovery and re-initialization for online UAV tracking with joint scale and aspect ratio optimization},
  author={Ding, Fangqiang and Fu, Changhong and Li, Yiming and Jin, Jin and Feng, Chen},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5970--5977},
  year={2020},
  organization={IEEE},
  pdf={https://arxiv.org/pdf/2008.03915.pdf},
  code={https://github.com/vision4robotics/JSAR-Tracker},
  abstract={Current unmanned aerial vehicle (UAV) visual tracking algorithms are primarily limited with respect to: (i) the kind of size variation they can deal with, (ii) the implementation speed which hardly meets the real-time requirement. In this work, a real-time UAV tracking algorithm with powerful size estimation ability is proposed. Specifically, the overall tracking task is allocated to two 2D filters: (i) translation filter for location prediction in the space domain, (ii) size filter for scale and aspect ratio optimization in the size domain. Besides, an efficient two-stage re-detection strategy is introduced for long-term UAV tracking tasks. Large-scale experiments on four UAV benchmarks demonstrate the superiority of the presented method which has computation feasibility on a low-cost CPU.}
}

@inproceedings{li2020keyfilter,
  abbr={ICRA},
  preview={kaot.png},
  title={Keyfilter-Aware Real-Time UAV Object Tracking},
  author={Li, Yiming and Fu, Changhong and Huang, Ziyuan and Zhang, Yinqiang and Pan, Jia},
  booktitle={IEEE International Conference on Robotics and Automation},
  pages={193--199},
  year={2020},
  organization={IEEE},
  arxiv={2003.05218},
  code={https://github.com/vision4robotics/KAOT-Tracker},
  pdf={https://ieeexplore.ieee.org/abstract/document/9196943},
  abstract={Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-ofthe-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.},
}

@INPROCEEDINGS{huang2019learning,
  abbr={ICCV},
  preview={arcf.png},
  author={Huang, Ziyuan and Fu, Changhong and Li, Yiming and Lin, Fuling and Lu, Peng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, 
  title={Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking}, 
  year={2019},
  pages={2891-2900},
  code={https://github.com/vision4robotics/ARCF-tracker},
  pdf={https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf},
  abstract={Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.}
  }





